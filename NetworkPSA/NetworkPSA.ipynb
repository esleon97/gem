{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import gzip\n",
    "import pickle5 as pickle\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import torchsnooper\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function gets the false positive rate, true positive rate, cutting threshold and area under curve using the given signal and background array\n",
    "def get_roc(sig,bkg):\n",
    "    testY = np.array([1]*len(sig) + [0]*len(bkg))\n",
    "    predY = np.array(sig+bkg)\n",
    "    auc = roc_auc_score(testY, predY)\n",
    "    fpr, tpr, thr = roc_curve(testY, predY)\n",
    "    return fpr,tpr,thr,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorDataset(Dataset):\n",
    "\n",
    "    def __init__(self,dsize=-1):\n",
    "        \n",
    "        DEP_dict = self.event_loader(\"DEP_P42575A.pickle\")\n",
    "        SEP_dict = self.event_loader(\"SEP_P42575A.pickle\")\n",
    "\n",
    "        if dsize == -1:\n",
    "            dsize = min(len(DEP_dict), len(SEP_dict))\n",
    "        \n",
    "        #Shuffle dataset and select #dsize event from DEP and SEP\n",
    "        np.random.shuffle(DEP_dict)\n",
    "        np.random.shuffle(SEP_dict)\n",
    "        DEP_dict = DEP_dict[:dsize]\n",
    "        SEP_dict = SEP_dict[:dsize]\n",
    "        self.event_dict = DEP_dict + SEP_dict\n",
    "        self.label = ([1]*len(DEP_dict)) + ([0] * len(SEP_dict))\n",
    "        \n",
    "        self.size = len(self.event_dict)\n",
    "        print(self.size)\n",
    "        \n",
    "        #Get offset values:\n",
    "        self.max_offset = np.max(self.get_field_from_dict(DEP_dict,\"tstart\") + self.get_field_from_dict(SEP_dict,\"tstart\"))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        event = self.event_dict[idx]\n",
    "        wf = np.array(event[\"wf\"])\n",
    "        \n",
    "        #Correct for tOffset:\n",
    "        offset = self.max_offset - event[\"tstart\"]\n",
    "        initial_count = int(offset/event[\"period\"])\n",
    "        #Making sure all waveforms are of the same size of 3000 by cropping off the tail\n",
    "        final_count = 3000 + initial_count\n",
    "        wf = wf[initial_count:final_count]\n",
    "        \n",
    "        \n",
    "        #Normalize WF\n",
    "        wf -= np.average(wf[:100]) #Baseline subtraction using the first 100 samples\n",
    "        wf /= np.max(wf) #normalize waveform to an amplitude of 1.0\n",
    "        \n",
    "        avse = event[\"avse\"]\n",
    "        tdrift = event[\"tDrift\"]\n",
    "        \n",
    "        return wf, self.label[idx], avse\n",
    "        \n",
    "    def return_label(self):\n",
    "        return self.trainY\n",
    "\n",
    "    def return_detector_array(self):\n",
    "        return self.detector_name\n",
    "    \n",
    "    #Load event from .pickle file\n",
    "    def event_loader(self, address):\n",
    "        wf_list = []\n",
    "        with (open(address, \"rb\")) as openfile:\n",
    "            while True:\n",
    "#                 if len(wf_list) > 2000:\n",
    "#                     break\n",
    "                try:\n",
    "                   wf_list.append(pickle.load(openfile, encoding='latin1'))\n",
    "                except EOFError:\n",
    "                    break\n",
    "        return wf_list\n",
    "    \n",
    "    def get_field_from_dict(self, input_dict, fieldname):\n",
    "        field_list = []\n",
    "        for event in input_dict:\n",
    "            field_list.append(event[fieldname])\n",
    "        return field_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "def load_data(batch_size):\n",
    "\n",
    "    dataset = DetectorDataset()\n",
    "    validation_split = .3 #Split data set into training & testing with 7:3 ratio\n",
    "    shuffle_dataset = True\n",
    "    random_seed= 42222\n",
    "\n",
    "    #make sure we have the same amount of signal/bkg in the training/test dataset\n",
    "    division = 2\n",
    "    dataset_size = int(len(dataset)/division)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    train_indices += list(division*dataset_size - 1-np.array(train_indices))\n",
    "    val_indices += list(division*dataset_size- 1-np.array(val_indices))\n",
    "\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = data_utils.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, drop_last=True)\n",
    "    test_loader = data_utils.DataLoader(dataset, batch_size=batch_size,sampler=valid_sampler,  drop_last=True)\n",
    "\n",
    "    return train_loader,test_loader, dataset.return_detector_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The fully connected part of neural network\n",
    "class FCNet(nn.Module):\n",
    "    def __init__(self, last_unit):\n",
    "        super(FCNet, self).__init__()\n",
    "        \n",
    "        fc1, fc2, fc3, fc4 = (512, 256, 96, 32)\n",
    "        self.fcnet = nn.Sequential(\n",
    "            torch.nn.Linear(fc1, fc2),\n",
    "            torch.nn.BatchNorm1d(fc2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(fc2, fc3),\n",
    "            torch.nn.BatchNorm1d(fc3),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(fc3, fc4),\n",
    "            torch.nn.BatchNorm1d(fc4),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(fc4, last_unit),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fcnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The CNN based model:\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,detector_len):\n",
    "        super(CNN, self).__init__()\n",
    "    \n",
    "        conv1, conv2, conv3, conv4 = (16,24,32,48)\n",
    "        self.fcnet = FCNet(1)\n",
    "        self.CNNBackbone = nn.Sequential(\n",
    "            torch.nn.Conv1d(1,conv1,16),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "            torch.nn.BatchNorm1d(conv1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Conv1d(conv1,conv2,8),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "            torch.nn.BatchNorm1d(conv2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Conv1d(conv2,conv3,4),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "            torch.nn.BatchNorm1d(conv3),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Conv1d(conv3,conv4,4),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "            torch.nn.BatchNorm1d(conv4),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout()\n",
    "        )\n",
    "        self.fc1 = torch.nn.Linear(183*conv4, 512)\n",
    "        self.detector_len = detector_len\n",
    "    \n",
    "    #@torchsnooper.snoop()\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.CNNBackbone(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fcnet(x)\n",
    "#         x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def get_detector_embedding(self):\n",
    "        return self.embedding(torch.arange(self.detector_len).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The RNN based model:\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        feed_in_dim = 512\n",
    "        self.seq_len = 1000\n",
    "        self.seg = 3\n",
    "        self.RNNLayer = torch.nn.GRU(input_size = self.seg, hidden_size = feed_in_dim//2, batch_first=True)\n",
    "        self.fcnet = FCNet(1)\n",
    "        self.attention_weight = Parameter(torch.empty(self.seq_len,feed_in_dim//2).uniform_(-0.1, 0.1))\n",
    "        self.norm = torch.nn.BatchNorm1d(feed_in_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,self.seq_len,self.seg)\n",
    "        output, hidden = self.RNNLayer(x)\n",
    "        hidden =  hidden[-1]\n",
    "        \n",
    "        #Attention Mechanism\n",
    "        hidden_attention = hidden.unsqueeze(1).expand(*output.size()) #[batch, 1, channel] -> [batch, seq_len, channel]\n",
    "        w_attention = self.attention_weight.unsqueeze(0).expand(*output.size()) #[1, seq_len, channel] -> [batch, seq_len, channel]\n",
    "        attention_score = torch.softmax(output * w_attention * hidden_attention,dim=1) #Softmax over seq_len dimension\n",
    "        context = torch.sum(attention_score * output,dim=1) #Sum over seq_len dimension with attention score multiplied to output\n",
    "        x = self.fcnet(torch.cat([context,hidden],dim=-1)) #concatenate context vector with last hidden state output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader, test_loader, det_array = load_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This feeds the waveform into classifier and get sigmoid output for signal and background events\n",
    "def get_sigmoid(waveform_in, labels_in ,classifier_in):\n",
    "    waveform_in = waveform_in.to(DEVICE)\n",
    "    labels_in = labels_in.to(DEVICE).float()\n",
    "    outputs_in  = classifier_in(waveform_in)\n",
    "\n",
    "    lb_data_in = labels_in.cpu().data.numpy().flatten()\n",
    "    outpt_data_in = outputs_in.cpu().data.numpy().flatten()\n",
    "\n",
    "    signal_in = np.argwhere(lb_data_in == 1.0)\n",
    "    bkg_in = np.argwhere(lb_data_in == 0.0)\n",
    "\n",
    "    return list(outpt_data_in[signal_in].flatten()), list(outpt_data_in[bkg_in].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE =1*10**-4\n",
    "\n",
    "#Define both CNN and RNN\n",
    "RNNclassifier = RNN()\n",
    "CNNclassifier = CNN(len(det_array))\n",
    "\n",
    "RNNclassifier.to(DEVICE)\n",
    "CNNclassifier.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "print(\"#params\", sum(x.numel() for x in RNNclassifier.parameters()))\n",
    "\n",
    "RNNcriterion = torch.nn.BCEWithLogitsLoss() #BCEWithLogitsLoss does not require the last layer to be sigmoid\n",
    "RNNcriterion = RNNcriterion.to(DEVICE)\n",
    "\n",
    "CNNcriterion = torch.nn.BCEWithLogitsLoss()\n",
    "CNNcriterion = CNNcriterion.to(DEVICE)\n",
    "\n",
    "RNNoptimizer = torch.optim.AdamW(\n",
    "    RNNclassifier.parameters(),\n",
    "    lr=LEARNING_RATE)\n",
    "\n",
    "CNNoptimizer = torch.optim.AdamW(\n",
    "    CNNclassifier.parameters(),\n",
    "    lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (waveform, labels, avse) in enumerate(train_loader):\n",
    "        CNNclassifier.train()\n",
    "        RNNclassifier.train()\n",
    "        waveform = waveform.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).float()\n",
    "        labels = labels.view(-1,1)\n",
    "        \n",
    "        #Train RNN\n",
    "        RNNoutputs  = RNNclassifier(waveform)\n",
    "        RNNloss = RNNcriterion(RNNoutputs, labels)\n",
    "        \n",
    "        #Train CNN\n",
    "        CNNoutputs  = CNNclassifier(waveform)\n",
    "        CNNloss = CNNcriterion(CNNoutputs, labels)\n",
    "\n",
    "        CNNloss.backward()\n",
    "        CNNoptimizer.step()        # update parameters of net\n",
    "        CNNoptimizer.zero_grad()   # reset gradient\n",
    "        \n",
    "        RNNloss.backward()\n",
    "        RNNoptimizer.step()        # update parameters of net\n",
    "        RNNoptimizer.zero_grad()   # reset gradient\n",
    "\n",
    "    print('\\rEpoch [{0}/{1}], Iter [{2}/{3}] Loss: {4:.4f}'.format(\n",
    "        epoch+1, NUM_EPOCHS, i+1, len(train_loader),\n",
    "        RNNloss.item(), end=\"\"),end=\"\")\n",
    "    sigmoid_s_RNN = []\n",
    "    sigmoid_b_RNN = []\n",
    "    sigmoid_s_CNN = []\n",
    "    sigmoid_b_CNN = []\n",
    "    avse_s = []\n",
    "    avse_b = []\n",
    "\n",
    "    for waveform,labels,avse in tqdm(test_loader):\n",
    "\n",
    "        CNNclassifier.eval()\n",
    "        RNNclassifier.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sig_CNN, bkg_CNN = get_sigmoid(waveform, labels, CNNclassifier)\n",
    "            sig_RNN, bkg_RNN = get_sigmoid(waveform, labels, RNNclassifier)\n",
    "\n",
    "            lb_data = labels.cpu().data.numpy().flatten()\n",
    "            avse_data = avse.cpu().data.numpy().flatten()\n",
    "            \n",
    "            signal = np.argwhere(lb_data == 1.0)\n",
    "            bkg = np.argwhere(lb_data == 0.0)\n",
    "            \n",
    "            sigmoid_s_RNN += sig_RNN\n",
    "            sigmoid_b_RNN += bkg_RNN\n",
    "            sigmoid_s_CNN += sig_CNN\n",
    "            sigmoid_b_CNN += bkg_CNN\n",
    "            \n",
    "            avse_s += list(avse_data[signal].flatten())\n",
    "            avse_b += list(avse_data[bkg].flatten())\n",
    "\n",
    "    #Set the range of scatter plot from 5% to 95% quantile of sigmoid output\n",
    "    xlow = np.quantile(sigmoid_s_RNN+sigmoid_b_RNN,0.05)\n",
    "    xhi = np.quantile(sigmoid_s_RNN+sigmoid_b_RNN,0.95)\n",
    "    ylow = np.quantile(sigmoid_s_CNN + sigmoid_b_CNN,0.05)\n",
    "    yhi = np.quantile(sigmoid_s_CNN + sigmoid_b_CNN,0.95)\n",
    "\n",
    "    #Plot sigmoid output for DEP events\n",
    "    plt.scatter(sigmoid_s_RNN, sigmoid_s_CNN,s=1)\n",
    "    plt.title(\"Sigmoid Output of DEP\")\n",
    "    plt.xlim(left=xlow, right=xhi)\n",
    "    plt.ylim(bottom=ylow, top=yhi)\n",
    "    plt.xlabel(\"RNN Sigmoid Output\")\n",
    "    plt.ylabel(\"CNN Sigmoid Output\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Sigmoid_signal.png\",dpi=200)\n",
    "    plt.show()\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    #Plot sigmoid output for SEP events\n",
    "    plt.scatter(sigmoid_b_RNN, sigmoid_b_CNN,s=1)\n",
    "    plt.xlim(left=xlow, right=xhi)\n",
    "    plt.ylim(bottom=ylow, top=yhi)\n",
    "    plt.title(\"Sigmoid Output of SEP\")\n",
    "    plt.xlabel(\"RNN Sigmoid Output\")\n",
    "    plt.ylabel(\"CNN Sigmoid Output\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Sigmoid_bkg.png\",dpi=200)\n",
    "    plt.show()\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot the ROC curve for CNN, RNN and AvsE\n",
    "    fpr_cnn, tpr_cnn, thr_cnn, auc_cnn = get_roc(sigmoid_s_CNN, sigmoid_b_CNN)\n",
    "    fpr_rnn, tpr_rnn, thr_rnn, auc_rnn = get_roc(sigmoid_s_RNN, sigmoid_b_RNN)\n",
    "    fpr_avse, tpr_avse, thr_avse, auc_avse = get_roc(avse_s, avse_b)\n",
    "    rej_tpr = tpr_avse[np.argmin(np.abs(thr_avse+1.0))]\n",
    "    plt.plot(fpr_cnn,tpr_cnn,label=\"CNN AUC: %.3f SEP Remain: %.1f%%\"%(auc_cnn, fpr_cnn[np.argmin(np.abs(tpr_cnn-rej_tpr))]*100.0))\n",
    "    plt.plot(fpr_rnn,tpr_rnn,label=\"RNN AUC: %.3f SEP Remain: %.1f%%\"%(auc_rnn,fpr_rnn[np.argmin(np.abs(tpr_rnn-rej_tpr))]*100.0))\n",
    "    plt.plot(fpr_avse,tpr_avse,label=\"AvsE AUC: %.3f SEP Remain: %.1f%%\"%(auc_avse,fpr_avse[np.argmin(np.abs(thr_avse+1.0))]*100.0))\n",
    "    plt.legend()\n",
    "    plt.savefig(\"ROC.png\",dpi=200)\n",
    "    plt.show()\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n",
    "    #Save CNN and RNN models.\n",
    "    torch.save(CNNclassifier.state_dict(), 'CNN.pt')\n",
    "    torch.save(RNNclassifier.state_dict(), 'RNN.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-ngc-20.06-v0",
   "language": "python",
   "name": "pytorch-ngc-20.06-v0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
