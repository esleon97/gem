{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetworkPSA Analysis Using Recurrent Neural Network + Attention\n",
    "- Run on Cori PyTorch GPU kernels or LEGEND Kernel\n",
    "- You can also run it on Cori PyTorch CPU kernel, but the speed will be extremely slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import torchsnooper\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "These 2 parameters define the range of waveform we'd like to analyze\n",
    "'''\n",
    "LSPAN = 100 # number of time samples prior to t0\n",
    "HSPAN = 200 # number of time samples after t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function gets the false positive rate, true positive rate, cutting threshold and area under curve using the given signal and background array\n",
    "def get_roc(sig,bkg):\n",
    "    testY = np.array([1]*len(sig) + [0]*len(bkg))\n",
    "    predY = np.array(sig+bkg)\n",
    "    auc = roc_auc_score(testY, predY)\n",
    "    fpr, tpr, thr = roc_curve(testY, predY)\n",
    "    return fpr,tpr,thr,auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Object\n",
    "- Created by extracting Majorana data and save in `.pickle` format.\n",
    "- The content of the pickle file can be found in `WaveformExtraction.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dep=\"DEP_P42575A_10percent.pickle\", sep = \"SEP_P42575A_10percent.pickle\",dsize=-1):\n",
    "        \n",
    "        DEP_dict = self.event_loader(dep)\n",
    "        SEP_dict = self.event_loader(sep)\n",
    "\n",
    "        if dsize == -1:\n",
    "            dsize = min(len(DEP_dict), len(SEP_dict))\n",
    "        \n",
    "        #Shuffle dataset and select #dsize event from DEP and SEP\n",
    "        np.random.shuffle(DEP_dict)\n",
    "        np.random.shuffle(SEP_dict)\n",
    "        DEP_dict = DEP_dict[:dsize]\n",
    "        SEP_dict = SEP_dict[:dsize]\n",
    "        self.event_dict = DEP_dict + SEP_dict\n",
    "        self.label = ([1]*len(DEP_dict)) + ([0] * len(SEP_dict))\n",
    "        \n",
    "        self.size = len(self.event_dict)\n",
    "        print(self.size)\n",
    "        \n",
    "        #Get offset values:\n",
    "        self.max_offset = np.max(self.get_field_from_dict(DEP_dict,\"tstart\") + self.get_field_from_dict(SEP_dict,\"tstart\"))\n",
    "        \n",
    "        #Get all unique detector name:\n",
    "        self.detector_name = np.unique(self.get_field_from_dict(DEP_dict,\"detector\") + self.get_field_from_dict(SEP_dict,\"detector\"))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def build_scaler(self):\n",
    "        '''\n",
    "        '''\n",
    "        wf_array = []\n",
    "        for i in range(self.size):\n",
    "            wf_array.append(self.get_wf(i).reshape(1,-1))\n",
    "        wf_array = np.concatenate(wf_array,axis=0)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(wf_array)\n",
    "        return scaler\n",
    "    \n",
    "    def get_scaler(self):\n",
    "        return self.scaler\n",
    "    \n",
    "    def set_scaler(self,scaler):\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def get_wf(self,idx):\n",
    "        event = self.event_dict[idx]\n",
    "        wf = np.array(event[\"wf\"]).flatten()\n",
    "        midindex = event[\"t0\"]\n",
    "\n",
    "        #baseline subtraction\n",
    "        wf -= np.average(wf[:(midindex-50)])\n",
    "        \n",
    "        #Extract waveform from its t0\n",
    "        wfbegin = midindex - LSPAN\n",
    "        wfend = midindex + HSPAN\n",
    "        \n",
    "        wf = wf[wfbegin:wfend]\n",
    "        wf = (wf - np.min(wf)) / (np.max(wf) - np.min(wf))#rescale wf to between 0 and 1\n",
    "\n",
    "        \n",
    "        return wf\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        event = self.event_dict[idx]\n",
    "        wf = np.array(event[\"wf\"]).flatten()\n",
    "        midindex = event[\"t0\"]\n",
    "\n",
    "        wf = self.get_wf(idx)\n",
    "        \n",
    "        avse = event[\"avse\"]\n",
    "        tdrift = event[\"tDrift\"]\n",
    "        \n",
    "        return wf, self.label[idx], avse\n",
    "        \n",
    "    def return_label(self):\n",
    "        return self.trainY\n",
    "\n",
    "    def return_detector_array(self):\n",
    "        return self.detector_name\n",
    "    \n",
    "    #Load event from .pickle file\n",
    "    def event_loader(self, address):\n",
    "        wf_list = []\n",
    "        with (open(address, \"rb\")) as openfile:\n",
    "            while True:\n",
    "#                 if len(wf_list) > 2000:\n",
    "#                     break\n",
    "                try:\n",
    "                   wf_list.append(pickle.load(openfile, encoding='latin1'))\n",
    "                except EOFError:\n",
    "                    break\n",
    "        return wf_list\n",
    "    \n",
    "    def get_field_from_dict(self, input_dict, fieldname):\n",
    "        field_list = []\n",
    "        for event in input_dict:\n",
    "            field_list.append(event[fieldname])\n",
    "        return field_list\n",
    "    \n",
    "    def plot_offset_correction(self):\n",
    "        plt.subplot(211)\n",
    "        \n",
    "        plt.subplot(212)\n",
    "# next(iter(DetectorDataset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "def load_data(batch_size):\n",
    "\n",
    "    dataset = DetectorDataset()\n",
    "    test_dataset = DetectorDataset(dep=\"DEP_P42575A_Co56.pickle\")\n",
    "    validation_split = .3 #Split data set into training & testing with 7:3 ratio\n",
    "    shuffle_dataset = True\n",
    "    random_seed= 42222\n",
    "\n",
    "    #make sure we have the same amount of signal/bkg in the training/test dataset\n",
    "    division = 2\n",
    "    dataset_size = int(len(dataset)/division)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    train_indices += list(division*dataset_size - 1-np.array(train_indices))\n",
    "    val_indices += list(division*dataset_size- 1-np.array(val_indices))\n",
    "\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "    \n",
    "    test_dataset_size = int(len(test_dataset))\n",
    "    test_indices = list(range(test_dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(test_indices)\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "    train_loader = data_utils.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, drop_last=True)\n",
    "    test_loader = data_utils.DataLoader(test_dataset, batch_size=batch_size,sampler=valid_sampler,  drop_last=True)\n",
    "\n",
    "    return train_loader,test_loader, dataset.return_detector_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The fully connected part of neural network\n",
    "class FCNet(nn.Module):\n",
    "    def __init__(self, first_unit, last_unit):\n",
    "        super(FCNet, self).__init__()\n",
    "        \n",
    "        #Number of channels in each fully connected layers\n",
    "        fc1, fc2, fc3, fc4 = (first_unit, int(first_unit*0.5), int(first_unit*0.25), int(first_unit*0.1))\n",
    "        do = 0.2\n",
    "        self.fcnet = nn.Sequential(\n",
    "            torch.nn.Linear(fc1, fc2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(do),\n",
    "            torch.nn.Linear(fc2, fc3),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(do),\n",
    "            torch.nn.Linear(fc3, fc4),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(do),\n",
    "            torch.nn.Linear(fc4, last_unit),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fcnet(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Recurrent Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The RNN based model:\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,get_attention = False):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        bidirec = True    #Whether to use a bidirectional RNN\n",
    "        self.bidirec =bidirec\n",
    "        feed_in_dim = 512\n",
    "        self.seg = 1      #Segment waveform to reduce its length. If the original waveform is (2000,1), then segment it with self.seg=5 can reduce its length to (400,5)\n",
    "        self.emb_dim = 128\n",
    "        self.emb_tick = 1/500.0\n",
    "        self.embedding = nn.Embedding(int(1/self.emb_tick)+1,self.emb_dim)\n",
    "        self.seq_len = (HSPAN + LSPAN)//self.seg\n",
    "        if bidirec:\n",
    "            self.RNNLayer = torch.nn.GRU(input_size = self.emb_dim, hidden_size = feed_in_dim//2,num_layers=3, batch_first=True,bidirectional=True,dropout=0.5)\n",
    "            feed_in_dim *= 2\n",
    "        else:\n",
    "            self.RNNLayer = torch.nn.GRU(input_size = self.emb_dim, hidden_size = feed_in_dim//2,num_layers=3, batch_first=True,bidirectional=False,dropout=0.5)\n",
    "        self.fcnet = FCNet(feed_in_dim,1)\n",
    "        self.attention_weight = nn.Linear(feed_in_dim//2, feed_in_dim//2, bias=False) # When turning off the bias, an nn.Linear is pretty much a matrix multiplication\n",
    "        self.norm = torch.nn.BatchNorm1d(feed_in_dim//2)\n",
    "        self.get_attention = get_attention\n",
    "\n",
    "    # @torchsnooper.snoop()\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1,self.seq_len)\n",
    "        x = (x - x.min(dim=-1,keepdim=True)[0])/(x.max(dim=-1,keepdim=True)[0] - x.min(dim=-1,keepdim=True)[0])\n",
    "        x = (x/self.emb_tick).long()\n",
    "        x = self.embedding(x)\n",
    "        bsize = x.size(0)\n",
    "        output, hidden = self.RNNLayer(x)\n",
    "        if self.bidirec:\n",
    "            hidden =  hidden[-2:]\n",
    "            hidden = hidden.transpose(0,1).reshape(bsize,-1)\n",
    "        else:\n",
    "            hidden =  hidden[-1]\n",
    "        \n",
    "        \n",
    "        #Attention Mechanism\n",
    "        hidden_attention = hidden.unsqueeze(-1) #[batch, channel] -> [batch, channel, 1]\n",
    "        w_attention = self.attention_weight(output) # [batch, seq_len, channel] * [channel, channel] -> [batch, seq_len, channel]\n",
    "        w_attention = torch.einsum(\"ijl,ilm->ijm\",w_attention,hidden_attention).squeeze(-1)   # [batch, seq_len, channel] * [batch, channel, 1] -> [batch, seq_len, 1]\n",
    "        attention_score = torch.softmax(w_attention,dim=-1) #Softmax over seq_len dimension\n",
    "        if self.get_attention:\n",
    "            return attention_score\n",
    "        \n",
    "        context = torch.sum(attention_score.unsqueeze(-1).expand(*output.size()) * output,dim=1) #Sum over seq_len dimension with attention score multiplied to output\n",
    "        x = self.fcnet(torch.cat([context,hidden],dim=-1)) #concatenate context vector with last hidden state output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "BATCH_SIZE = 32\n",
    "train_loader, test_loader, det_array = load_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This feeds the waveform into classifier and get sigmoid output for signal and background events\n",
    "def get_sigmoid(waveform_in, labels_in ,classifier_in):\n",
    "    waveform_in = waveform_in.to(DEVICE)\n",
    "    labels_in = labels_in.to(DEVICE).float()\n",
    "    outputs_in  = classifier_in(waveform_in)\n",
    "\n",
    "    lb_data_in = labels_in.cpu().data.numpy().flatten()\n",
    "    outpt_data_in = outputs_in.cpu().data.numpy().flatten()\n",
    "\n",
    "    signal_in = np.argwhere(lb_data_in == 1.0)\n",
    "    bkg_in = np.argwhere(lb_data_in == 0.0)\n",
    "\n",
    "    return list(outpt_data_in[signal_in].flatten()), list(outpt_data_in[bkg_in].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE =0.1\n",
    "\n",
    "#Define RNN network\n",
    "RNNclassifier = RNN()\n",
    "\n",
    "RNNclassifier.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "print(\"#params\", sum(x.numel() for x in RNNclassifier.parameters()))\n",
    "\n",
    "RNNcriterion = torch.nn.BCEWithLogitsLoss() #BCEWithLogitsLoss does not require the last layer to be sigmoid\n",
    "RNNcriterion = RNNcriterion.to(DEVICE)\n",
    "\n",
    "# Warmup training scheme\n",
    "# This allows the attention mechanism to learn general features of data in first few epochs\n",
    "warmup_size = 4000 # Warm up step used in transformer paper\n",
    "print(\"Warmup Size: %d\"%(warmup_size))\n",
    "lmbda = lambda epoch: min((epoch+1)**-0.5, (epoch+1)*warmup_size**-1.5)\n",
    "RNNoptimizer = torch.optim.AdamW(RNNclassifier.parameters(),lr=LEARNING_RATE, betas=(0.9, 0.98),eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(RNNoptimizer, lr_lambda=lmbda)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (waveform, labels, avse) in enumerate(train_loader):\n",
    "        RNNclassifier.train()\n",
    "        waveform = waveform.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).float()\n",
    "        labels = labels.view(-1,1)\n",
    "        \n",
    "        #Train RNN\n",
    "        RNNoutputs  = RNNclassifier(waveform)\n",
    "        RNNloss = RNNcriterion(RNNoutputs, labels)\n",
    "        \n",
    "        RNNloss.backward()\n",
    "        RNNoptimizer.step()        # update parameters of net\n",
    "        RNNoptimizer.zero_grad()   # reset gradient\n",
    "        scheduler.step()\n",
    "\n",
    "    print('\\rEpoch [{0}/{1}], Iter [{2}/{3}] Loss: {4:.4f}'.format(\n",
    "        epoch+1, NUM_EPOCHS, i+1, len(train_loader),\n",
    "        RNNloss.item(), end=\"\"),end=\"\")\n",
    "    sigmoid_s_RNN = []\n",
    "    sigmoid_b_RNN = []\n",
    "    avse_s = []\n",
    "    avse_b = []\n",
    "\n",
    "    for waveform,labels,avse in tqdm(test_loader):\n",
    "\n",
    "        RNNclassifier.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sig_RNN, bkg_RNN = get_sigmoid(waveform, labels, RNNclassifier)\n",
    "\n",
    "            lb_data = labels.cpu().data.numpy().flatten()\n",
    "            avse_data = avse.cpu().data.numpy().flatten()\n",
    "            \n",
    "            signal = np.argwhere(lb_data == 1.0)\n",
    "            bkg = np.argwhere(lb_data == 0.0)\n",
    "            \n",
    "            sigmoid_s_RNN += sig_RNN\n",
    "            sigmoid_b_RNN += bkg_RNN\n",
    "            \n",
    "            avse_s += list(avse_data[signal].flatten())\n",
    "            avse_b += list(avse_data[bkg].flatten())\n",
    "\n",
    "    #Set the range of scatter plot from 5% to 95% quantile of sigmoid output\n",
    "    xlow = np.quantile(sigmoid_s_RNN+sigmoid_b_RNN,0.05)\n",
    "    xhi = np.quantile(sigmoid_s_RNN+sigmoid_b_RNN,0.95)\n",
    "\n",
    "    # Plot the ROC curve for RNN and AvsE\n",
    "    fpr_rnn, tpr_rnn, thr_rnn, auc_rnn = get_roc(sigmoid_s_RNN, sigmoid_b_RNN)\n",
    "    fpr_avse, tpr_avse, thr_avse, auc_avse = get_roc(avse_s, avse_b)\n",
    "    rej_tpr = tpr_avse[np.argmin(np.abs(thr_avse+1.0))]\n",
    "    plt.plot(fpr_rnn,tpr_rnn,label=\"RNN AUC: %.3f SEP Remain: %.1f%%\"%(auc_rnn,fpr_rnn[np.argmin(np.abs(tpr_rnn-rej_tpr))]*100.0))\n",
    "    plt.plot(fpr_avse,tpr_avse,label=\"AvsE AUC: %.3f SEP Remain: %.1f%%\"%(auc_avse,fpr_avse[np.argmin(np.abs(thr_avse+1.0))]*100.0))\n",
    "    plt.legend()\n",
    "    plt.savefig(\"ROC.png\",dpi=200)\n",
    "    plt.show()\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n",
    "    #Save CNN and RNN models.\n",
    "    torch.save(RNNclassifier.state_dict(), 'RNN.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "- Confusion plot comparing AvsE and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion plot of A vs. E and RNN classifier\n",
    "xlow = np.quantile(sigmoid_s_RNN+sigmoid_b_RNN,0)\n",
    "xhi = np.quantile(sigmoid_s_RNN+sigmoid_b_RNN,1.00)\n",
    "ylow = -10\n",
    "yhi =2\n",
    "\n",
    "threshold_rnn = thr_rnn[np.argmin(np.abs(tpr_rnn-tpr_avse[np.argmin(np.abs(thr_avse+1.0))]))]\n",
    "\n",
    "#Plot sigmoid output for DEP events\n",
    "plt.hist2d(sigmoid_s_RNN, avse_s,bins = (np.linspace(xlow,xhi,100),np.linspace(ylow,yhi,100)),cmap=\"PuRd\",norm=matplotlib.colors.LogNorm())\n",
    "plt.axhline(y=-1,color=\"blue\")\n",
    "plt.axvline(x=threshold_rnn,color=\"blue\")\n",
    "plt.title(\"Network Output of DEP\")\n",
    "plt.xlabel(\"RNN Output\")\n",
    "plt.ylabel(\"AvsE Corrected\")\n",
    "# plt.legend()\n",
    "plt.savefig(\"AR_signal.png\",dpi=200)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "\n",
    "#Plot sigmoid output for SEP events\n",
    "plt.hist2d(sigmoid_b_RNN, avse_b,bins = (np.linspace(xlow,xhi,100),np.linspace(ylow,yhi,100)),cmap=\"PuRd\",norm=matplotlib.colors.LogNorm())\n",
    "plt.axhline(y=-1,color=\"blue\")\n",
    "plt.axvline(x=threshold_rnn,color=\"blue\")\n",
    "plt.title(\"Network Output of SEP\")\n",
    "plt.xlabel(\"RNN Output\")\n",
    "plt.ylabel(\"AvsE Corrected\")\n",
    "# plt.legend()\n",
    "plt.savefig(\"AR_bkg.png\",dpi=200)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- network output as a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network output for DEP events\n",
    "plt.hist(sigmoid_s_RNN, bins = np.linspace(xlow,xhi,100),color=\"red\",histtype=\"step\",label=\"DEP\")\n",
    "plt.hist(sigmoid_b_RNN, bins = np.linspace(xlow,xhi,100),color=\"blue\",histtype=\"step\",label=\"SEP\")\n",
    "plt.title(\"RNN output\")\n",
    "plt.xlabel(\"RNN Sigmoid Output\")\n",
    "plt.ylabel(\"RNN output\")\n",
    "plt.legend()\n",
    "plt.savefig(\"RNN1d.png\",dpi=200)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plotting the attention score of give events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(waveform, attscore,bkg=False):\n",
    "    '''\n",
    "    This function plots the attention score distribution on given waveform\n",
    "    waveform: the vector of original waveform\n",
    "    attscore: the attention score obtained from the RNN\n",
    "    '''\n",
    "    from matplotlib import cm\n",
    "    from matplotlib import gridspec\n",
    "    colormap_normal = cm.get_cmap(\"cool\")\n",
    "    \n",
    "    waveform=np.array(waveform)\n",
    "    attscore = np.array(attscore)\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[8,1]) \n",
    "\n",
    "    plt.subplot(gs[0])\n",
    "    rescale = lambda y: (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "    len_wf = len(waveform)\n",
    "    print(np.linspace(0,len_wf,len_wf).shape,waveform.shape, rescale(attscore).shape)\n",
    "    plt.bar(np.linspace(0,len_wf,len_wf),waveform,width=1.5, color=colormap_normal(rescale(attscore)))\n",
    "    plt.xlabel(\"Time Sample\")\n",
    "    plt.ylabel(\"ADC Counts\")\n",
    "\n",
    "    loss_ax_scale = fig.add_subplot(gs[1])\n",
    "    loss_ax_scale.set_xticks([])\n",
    "    loss_ax_scale.tick_params(length=0)\n",
    "    plt.yticks([1,72], [\"High Attention\", \"Low Attention\"],rotation=90)  # Set text labels and properties.\n",
    "\n",
    "    # loss_ax_scale.set_yticks([1.0,0.0])\n",
    "    # loss_ax_scale.set_yticklabels([\"High Attention\", \"Low Attention\"],rotation=90)\n",
    "    loss_scale = np.linspace(1.0, 0.0, 100)\n",
    "\n",
    "    for i in range(0,1):\n",
    "        loss_scale = np.vstack((loss_scale,loss_scale))\n",
    "    loss_scale = loss_ax_scale.imshow(np.transpose(loss_scale),cmap=colormap_normal, interpolation='nearest')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if bkg:\n",
    "        plt.savefig(\"att_nhit_bkg.png\",dpi=200)\n",
    "    else:\n",
    "        plt.savefig(\"att_nhit_sig.png\",dpi=200)\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the RNN to attention score mode\n",
    "attentionRNN = RNN(True)\n",
    "attentionRNN.to(DEVICE).double()\n",
    "model_dict = attentionRNN.state_dict()\n",
    "pretrained_dict = torch.load('RNN.pt',map_location='cpu')\n",
    "model_dict.update(pretrained_dict) \n",
    "attentionRNN.load_state_dict(pretrained_dict)\n",
    "\n",
    "attentionRNN.eval()\n",
    "\n",
    "wf = next(iter(test_loader))[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    waveform = torch.tensor(wf).to(DEVICE)\n",
    "    attention  = attentionRNN(waveform)\n",
    "    print(attention.size())\n",
    "    \n",
    "    ibatch=0\n",
    "    wf = waveform[ibatch]#.view(600,3)[:,0]\n",
    "    attention = attention[ibatch]\n",
    "    plot_attention(wf.cpu().data.numpy().flatten(), attention.cpu().data.numpy().flatten())\n",
    "    assert 0\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legend-kernel",
   "language": "python",
   "name": "legend-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
